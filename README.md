# Ingestion de donnÃ©es depuis datos.gob.es

Ce projet implÃ©mente un pipeline dâ€™**ingestion de donnÃ©es** depuis le portail [datos.gob.es](https://datos.gob.es), qui expose les jeux de donnÃ©es publics espagnols via une API.  
Les donnÃ©es rÃ©cupÃ©rÃ©es sont stockÃ©es dans une base **SQLite** et exportÃ©es au format **CSV**.

---

## ğŸ“Œ Quâ€™est-ce que la Data Ingestion ?

La **data ingestion** est le processus qui consiste Ã  collecter des donnÃ©es depuis une ou plusieurs sources (APIs, bases, fichiers, flux temps rÃ©el, etc.) et Ã  les intÃ©grer dans un systÃ¨me cible (base de donnÃ©es, data lake, entrepÃ´t de donnÃ©es).  

Il existe deux modes principaux :

- **Batch ingestion** : les donnÃ©es sont collectÃ©es pÃ©riodiquement (par lots). Exemple : lancer ce script une fois par jour pour mettre Ã  jour les jeux de donnÃ©es.
- **Real-time ingestion** : les donnÃ©es sont collectÃ©es en continu, Ã  mesure quâ€™elles sont produites (flux Kafka, WebSockets, etc.). Exemple : surveiller un flux dâ€™Ã©vÃ©nements en direct et lâ€™intÃ©grer immÃ©diatement.

Dans ce projet, lâ€™ingestion est rÃ©alisÃ©e en **batch** (pages successives dâ€™API).

---

## âš™ï¸ FonctionnalitÃ©s principales

- RÃ©cupÃ©ration des jeux de donnÃ©es et de leurs distributions depuis lâ€™API de datos.gob.es.
- Normalisation des champs texte.
- Stockage en base **SQLite** :
  - Table `datasets`  
  - Table `distributions`  
  - Table `geo_taxonomy` (taxonomie NTI des couvertures gÃ©ographiques)
- Export des donnÃ©es au format **CSV** dans le dossier `./data`.

---

## ğŸ“š Description des fonctions

### Connexion et schÃ©ma de base
- **`mk_conn(db_path="datosgob.db")`**  
  CrÃ©e une base SQLite, supprime les anciennes tables si elles existent, et dÃ©finit la structure des tables :
  - `datasets`
  - `distributions`
  - `geo_taxonomy`

---

### RÃ©cupÃ©ration et normalisation
- **`fetch_page(page=0)`**  
  Interroge lâ€™API datos.gob.es pour une page donnÃ©e, avec paramÃ¨tres `_pageSize` et `_page`.

- **`normalize_text(value)`**  
  Transforme un champ (string, dict, list) en une chaÃ®ne lisible.  
  Permet dâ€™unifier les formats hÃ©tÃ©rogÃ¨nes de lâ€™API.

---

### Extraction des donnÃ©es
- **`extract_dataset(item)`**  
  Construit un dictionnaire Python normalisÃ© pour un dataset (id, titre, description, dates, Ã©diteur, JSON brut).

- **`extract_distributions(item, dataset_id)`**  
  Extrait les distributions associÃ©es Ã  un dataset (id, format, URLs, JSON brut).

---

### Sauvegarde
- **`save_to_db(conn, dataset, distributions)`**  
  InsÃ¨re ou met Ã  jour un dataset et ses distributions dans SQLite.

- **`insert_geo_entities(conn)`**  
  InsÃ¨re dans `geo_taxonomy` un ensemble dâ€™entitÃ©s NTI (pays, rÃ©gions, provinces).  
  Peut Ãªtre adaptÃ© pour rÃ©cupÃ©rer directement depuis lâ€™API NTI.

---

### Export
- **`export_csv(conn, out_dir="data")`**  
  Exporte les tables `datasets` et `distributions` en CSV.

- **`export_geo_csv(conn, out_dir="data")`**  
  Exporte la table `geo_taxonomy` en CSV.

---

### Orchestration
- **`main(max_pages=10)`**  
  Pipeline principal :
  1. Initialise la base (`mk_conn`)  
  2. Charge la taxonomie NTI (`insert_geo_entities`)  
  3. Parcourt les pages de lâ€™API (`fetch_page`)  
  4. Extrait et sauvegarde les datasets/distributions  
  5. Exporte en CSV (`export_csv`, `export_geo_csv`)

---

## ğŸš€ ExÃ©cution

```bash
python ingest_datosgob.py
